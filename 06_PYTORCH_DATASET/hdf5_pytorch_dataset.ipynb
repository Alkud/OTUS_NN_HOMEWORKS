{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import reshape as t_reshape\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tempfile\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс HDF5 Dataset для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем набор данных в виде файла tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_from_url(url_name, file_name, read_size=1024):\n",
    "    with urllib.request.urlopen(url_name) as web_handle:\n",
    "        with open(file=file_name, mode='wb') as target_file:\n",
    "            while True:\n",
    "                data = web_handle.read(read_size)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                target_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_targz_file = 'cifar10_raw.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_from_url(cifar10_url, cifar_targz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Источник: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        labeled_data = pickle.load(fo, encoding='bytes')\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распаковывем содержимое архива и сохраняем в файл hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cifar10_to_hdf5(targz_name, h5_name):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    with tarfile.open(targz_name, 'r:gz') as tar:\n",
    "        # распаковываем файлы архива, имеющие в названии '_batch'\n",
    "        for tarinfo in tar:            \n",
    "            if tarinfo.isreg() and '_batch' in tarinfo.name:\n",
    "                print(f'Extracting {tarinfo.name} file...')\n",
    "            else:\n",
    "                continue\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                tar.extract(tarinfo, Path(temp_dir))\n",
    "                with open(Path(temp_dir).joinpath(tarinfo.name).absolute(), 'rb') as temp_file:\n",
    "                    batch_dict = pickle.load(temp_file, encoding='bytes')\n",
    "                    batch_data = batch_dict[b'data']\n",
    "                    batch_labels = batch_dict[b'labels']\n",
    "                    # массив тестовых данных откладываем отдельно\n",
    "                    if 'test' in tarinfo.name:\n",
    "                        test_data.append(batch_data)\n",
    "                        test_labels.append(batch_labels)\n",
    "                    # а массивы обучающих данных собираем вместе                         \n",
    "                    else:\n",
    "                        train_data.append(batch_data)\n",
    "                        train_labels.append(batch_labels)\n",
    "    full_train_data = np.concatenate(train_data, axis=0)\n",
    "    full_train_labels = np.concatenate(train_labels, axis=0)    \n",
    "    full_test_data = np.concatenate(test_data, axis=0)\n",
    "    full_test_labels = np.concatenate(test_labels, axis=0)\n",
    "    with h5py.File(h5_name, 'w') as h5_file:\n",
    "        # создаём hdf5 файл и создаём в нём две группы - для тестовых и обучающих выборок\n",
    "        h5_file.create_group('train')\n",
    "        h5_file.create_group('test')\n",
    "        # в каждой группе создаём два набора данных - для изображений и для меток\n",
    "        h5_file['train'].create_dataset('data', data=full_train_data)\n",
    "        h5_file['train'].create_dataset('labels', data=full_train_labels)\n",
    "        h5_file['test'].create_dataset('data', data=full_test_data)\n",
    "        h5_file['test'].create_dataset('labels', data=full_test_labels)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cifar-10-batches-py/data_batch_4 file...\n",
      "Extracting cifar-10-batches-py/test_batch file...\n",
      "Extracting cifar-10-batches-py/data_batch_3 file...\n",
      "Extracting cifar-10-batches-py/data_batch_2 file...\n",
      "Extracting cifar-10-batches-py/data_batch_5 file...\n",
      "Extracting cifar-10-batches-py/data_batch_1 file...\n"
     ]
    }
   ],
   "source": [
    "extract_cifar10_to_hdf5(cifar_targz_file, 'cifar_10.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаляем исходный архив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Remove tar.gz file\n",
    "\n",
    "cifar_path = Path(cifar_targz_file)\n",
    "if cifar_path.exists():\n",
    "    cifar_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наследуем наш класс от torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5_Dataset(Dataset):\n",
    "    '''\n",
    "    Набор данных в формате файла HDF5\n",
    "    Параметры:\n",
    "    hdf5_file_name (string) : имя файла, содержащего набор данных\n",
    "    train (bool) : True если нужна обучающая выборка, False - если тестовая\n",
    "    transform (callable) : преобразование данных\n",
    "    '''    \n",
    "    def __init__(self, hdf5_file_name, train, transform=None):                \n",
    "        super().__init__()\n",
    "        assert(Path(hdf5_file_name).exists())\n",
    "        self.__h5_file = h5py.File(hdf5_file_name, 'r')        \n",
    "        assert('train' in self.__h5_file.keys() and 'test' in self.__h5_file.keys())        \n",
    "        assert(self.__h5_file['train']['data'].len() == self.__h5_file['train']['labels'].len())\n",
    "        \n",
    "        if train:\n",
    "            self.__data = self.__h5_file['train']['data']\n",
    "            self.__data_size = self.__data.len()\n",
    "            self.__labels = self.__h5_file['train']['labels']\n",
    "        else:\n",
    "            self.__data = self.__h5_file['test']['data']\n",
    "            self.__data_size = self.__data.len()\n",
    "            self.__labels = self.__h5_file['test']['labels']            \n",
    "        \n",
    "        self.__transform = transform\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.__data_size\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        np_data = np.zeros(self.__data.shape)\n",
    "        self.__data.read_direct(np_data)\n",
    "        return np_data\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.__labels\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index >= self.__data_size:\n",
    "            raise IndexError('Item index out of range!')\n",
    "        item = self.__data[index]\n",
    "        if self.__transform is not None:\n",
    "            item = self.__transform(item)\n",
    "        label = np.int64(self.__labels[index])\n",
    "        \n",
    "        return (item, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.__data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HDF5_Dataset('cifar_10.hdf5', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные изображения хранятся в виде одномерных массивов размером 3072.\n",
    "### Этот размер получается так: 3 канала (цветное изображерие) по 1024 пикселя (32*32).\n",
    "### При этом, первый 1024 значения - пиксели красного канала, и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_data = test_dataset.data.reshape(-1,3,32,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделяем каналы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_data = reshaped_data.transpose([0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формируем цветное изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисляем средние значения и стандартные отклонения поканально по всей обучающей выборке.\n",
    "### Эти значения нужны для последующей нормализации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49139967861519607"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 0].mean() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24703223246328176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 0].std() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48215840839460783"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 1].mean() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2434851280000552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 1].std() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44653091444546567"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 2].mean() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26158784172796484"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_data[:, :, :, 2].std() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(transposed_data[1115] / 255);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цепочка трансофрмаций: преобразование размерностей, приведение значений к диапазону [0, 1] и нормализация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: np.reshape(x, (3, 32, 32)).transpose([1, 2, 0])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_transform(reshaped_data[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим объекты для загрузки тренировочных и тестовых данных из нашего набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_h5_train_dataset = HDF5_Dataset('cifar_10.hdf5', train=True, transform=cifar10_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=cifar5_h5_train_dataset, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_h5_test_dataset = HDF5_Dataset('cifar_10.hdf5', train=False, transform=cifar10_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset=cifar5_h5_test_dataset, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим свёрточную сеть с использованием полученных наборов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, lr=1e-4, l2=0.):        \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3), # 32x30x30\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3), # 128x28x28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 128x14x14\n",
    "\n",
    "            # block 2\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3), # 256x12x12\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3), # 256x10x10\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 256x5x5\n",
    "        )\n",
    "        \n",
    "        self.flat = nn.Flatten() # 256*25 = 6400\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(6400, 3200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(3200, 800),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(800, 10)\n",
    "        )\n",
    "        \n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x) # 256x5x5        \n",
    "        x = self.flat(x) # 6400        \n",
    "        x = self.fc_layer(x) # 10        \n",
    "        x = F.log_softmax(x, dim=1) # 10        \n",
    "        return x\n",
    "\n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(epoch, model, loader, log=None):\n",
    "    train_size = len(loader.sampler)\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(loader):        \n",
    "        model.optim.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss(output, target)\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        loss.backward()\n",
    "        model.optim.step()\n",
    "            \n",
    "        if batch_idx % 100 == 0:\n",
    "            line = f'Train epoch {epoch} [{batch_idx * len(data):05d}/{train_size}] '\n",
    "            loss = f'\\tloss: {model._loss.item():.6f}'\n",
    "            print(line + loss)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = f'Train epoch {epoch} [{batch_idx * len(data):05d}/{train_size}] '\n",
    "        loss = f'\\tloss: {model._loss.item():.6f}'\n",
    "        if log is not None:\n",
    "            log.append(model._loss)\n",
    "        print(line + loss)\n",
    "\n",
    "    train_accuracy = float(correct) / len(loader.dataset)\n",
    "    line = f'\\tTrain accuracy: {train_accuracy}'    \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, loader, log=None):\n",
    "    \n",
    "    avg_lambda = lambda l: f'loss: {l:.4f}'\n",
    "    acc_lambda = lambda c, p: f'accuracy: {c}/{len(loader.dataset)} ({p:.0f}%)'\n",
    "    line = lambda l, c, p: avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = len(loader.sampler)\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = model(data)            \n",
    "            test_loss += model.loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()                \n",
    "    \n",
    "    test_loss /= len(loader.dataset)\n",
    "    test_accuracy = float(correct) / len(loader.dataset)\n",
    "    line = line(test_loss, correct, test_accuracy*100)\n",
    "    report = '\\tTest set:' + line\n",
    "    print(report,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cifar_classifier = CNN()\n",
    "train_log = []\n",
    "test_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 [00000/50000] \tloss: 2.155929\n",
      "Train epoch 1 [25000/50000] \tloss: 1.303649\n",
      "Train epoch 1 [50000/50000] \tloss: 1.118451\n",
      "\tTrain accuracy: 0.50272\n",
      "\tTest set:loss: 1.0924\taccuracy: 6083/10000 (61%) \n",
      "\n",
      "Train epoch 2 [00000/50000] \tloss: 1.074768\n",
      "Train epoch 2 [25000/50000] \tloss: 0.941617\n",
      "Train epoch 2 [50000/50000] \tloss: 0.727696\n",
      "\tTrain accuracy: 0.65774\n",
      "\tTest set:loss: 0.9143\taccuracy: 6781/10000 (68%) \n",
      "\n",
      "Train epoch 3 [00000/50000] \tloss: 0.835358\n",
      "Train epoch 3 [25000/50000] \tloss: 0.749201\n",
      "Train epoch 3 [50000/50000] \tloss: 0.731529\n",
      "\tTrain accuracy: 0.7255\n",
      "\tTest set:loss: 0.8299\taccuracy: 7134/10000 (71%) \n",
      "\n",
      "Train epoch 4 [00000/50000] \tloss: 0.678310\n",
      "Train epoch 4 [25000/50000] \tloss: 0.568471\n",
      "Train epoch 4 [50000/50000] \tloss: 0.659380\n",
      "\tTrain accuracy: 0.78254\n",
      "\tTest set:loss: 0.7344\taccuracy: 7415/10000 (74%) \n",
      "\n",
      "Train epoch 5 [00000/50000] \tloss: 0.427943\n",
      "Train epoch 5 [25000/50000] \tloss: 0.482206\n",
      "Train epoch 5 [50000/50000] \tloss: 0.447646\n",
      "\tTrain accuracy: 0.83094\n",
      "\tTest set:loss: 0.7224\taccuracy: 7514/10000 (75%) \n",
      "\n",
      "Train epoch 6 [00000/50000] \tloss: 0.389295\n",
      "Train epoch 6 [25000/50000] \tloss: 0.390179\n",
      "Train epoch 6 [50000/50000] \tloss: 0.385670\n",
      "\tTrain accuracy: 0.86842\n",
      "\tTest set:loss: 0.7570\taccuracy: 7466/10000 (75%) \n",
      "\n",
      "Train epoch 7 [00000/50000] \tloss: 0.333621\n",
      "Train epoch 7 [25000/50000] \tloss: 0.284660\n",
      "Train epoch 7 [50000/50000] \tloss: 0.288126\n",
      "\tTrain accuracy: 0.90676\n",
      "\tTest set:loss: 0.7229\taccuracy: 7744/10000 (77%) \n",
      "\n",
      "Train epoch 8 [00000/50000] \tloss: 0.208152\n",
      "Train epoch 8 [25000/50000] \tloss: 0.203512\n",
      "Train epoch 8 [50000/50000] \tloss: 0.154937\n",
      "\tTrain accuracy: 0.94384\n",
      "\tTest set:loss: 0.7498\taccuracy: 7706/10000 (77%) \n",
      "\n",
      "Train epoch 9 [00000/50000] \tloss: 0.088690\n",
      "Train epoch 9 [25000/50000] \tloss: 0.091993\n",
      "Train epoch 9 [50000/50000] \tloss: 0.068661\n",
      "\tTrain accuracy: 0.97\n",
      "\tTest set:loss: 0.8073\taccuracy: 7757/10000 (78%) \n",
      "\n",
      "Train epoch 10 [00000/50000] \tloss: 0.062854\n",
      "Train epoch 10 [25000/50000] \tloss: 0.078545\n",
      "Train epoch 10 [50000/50000] \tloss: 0.064731\n",
      "\tTrain accuracy: 0.98648\n",
      "\tTest set:loss: 0.8403\taccuracy: 7769/10000 (78%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    cnn_cifar_classifier.train()\n",
    "    train_classifier(epoch, cnn_cifar_classifier, train_dataloader, log=train_log)\n",
    "    cnn_cifar_classifier.eval()\n",
    "    test_classifier(cnn_cifar_classifier, test_dataloader, log=test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные преобразуются корректно, сеть обучается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
