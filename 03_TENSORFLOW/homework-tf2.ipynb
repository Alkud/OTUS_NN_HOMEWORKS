{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "### Д/з из четырех пунктов:\n",
    "* Улучшение `fit_generator`\n",
    "* Сравнение двух ReLU (разные активации)\n",
    "* Испорченный батч-норм \n",
    "* \"Сырые\" данные. \n",
    "\n",
    "### Что нужно сделать\n",
    "* Следовать инструкциям в каждом из пунктов.\n",
    "* Результатами вашей работы будет ноутбук с доработанным кодом + архив с директорией с логами `tensorboard` `logs/`, в который вы запишите результаты экспериментов. Подробности в инструкциях ниже.\n",
    "* Можно и нужно пользоваться кодом из файла `utils`, **но** весь код модифицируйте, пожалуйста, в ноутбуках! Так мне будет проще проверять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка tensorboard в ноутбук**\n",
    "\n",
    "Можете попробовать использовать его так на свой страх и риск :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт слоев для д/з"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BatchNormFlawed, Dense, DenseSmart, Sequential, MNISTSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "> Здесь ничего менять не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tr, y_tr), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(X_tr, y_tr, 128)\n",
    "test_seq = MNISTSequence(X_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno, os, stat, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_remove_readonly(func, path, exc):\n",
    "    excvalue = exc[1]\n",
    "    if func in (os.rmdir, os.remove) and excvalue.errno == errno.EACCES:\n",
    "        os.chmod(path, stat.S_IRWXU| stat.S_IRWXG| stat.S_IRWXO) # 0777\n",
    "        func(path)\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Улучшение fit_generator\n",
    "\n",
    "Улучшите метод `fit_generator` так, чтобы он:\n",
    "* Записывал значения градиентов для всех переменных при помощи `tf.summary.histogram` \n",
    "* Записывал значения ошибки и метрики на валидации с помощью `tf.summary.scalar`\n",
    "\n",
    "Затем сделайте monkey patch класса sequential обновленным методом (следующая ячейка за методом `fit_generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generator(self, train_seq, eval_seq, epoch, loss, optimizer, writer=None, log_freq=50):\n",
    "    history = dict(train=list(), val=list())\n",
    "\n",
    "    train_loss_results = list()\n",
    "    val_loss_results = list()\n",
    "\n",
    "    train_accuracy_results = list()\n",
    "    val_accuracy_results = list()\n",
    "\n",
    "    step = 0\n",
    "    for e in range(epoch):\n",
    "        p = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
    "\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        epoch_accuracy_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        for x, y in train_seq:\n",
    "            with tf.GradientTape() as tape:\n",
    "                \"\"\"\n",
    "                Обратите внимание! Если записывать гистограмму каждый шаг,\n",
    "                обучение будет идти очень медленно. Поэтому записываем данные \n",
    "                каждый i-й шаг.\n",
    "                \"\"\"\n",
    "                if step % log_freq == 0:\n",
    "                    prediction = self._forward(x, writer, step)\n",
    "                else:\n",
    "                    prediction = self._forward(x)\n",
    "                loss_value = loss(y, prediction)\n",
    "                     \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте запись градиентов в гистограммы                    #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "        \n",
    "            gradients = tape.gradient(loss_value, self._trainable_variables)\n",
    "            \n",
    "            if step % log_freq == 0:\n",
    "                \"\"\"\n",
    "                Пример того, как можно дать всем градиентам уникальные имена. \n",
    "                Обратите внимание! Создание grad_names лучше вынести из цикла,\n",
    "                чтобы не пересоздавать список на каждом шаге! \n",
    "                \"\"\"\n",
    "                grad_names = list()\n",
    "                for layer in self._layers:\n",
    "                    for var_num, var in enumerate(layer.get_trainable()):\n",
    "                        grad_names.append(f\"grad_{layer.name}_{var_num}\")\n",
    "                # сохраняем значения градиентов  \n",
    "                if writer is not None:\n",
    "                    with writer.as_default():\n",
    "                        for name, grad in zip(grad_names, gradients):                    \n",
    "                            tf.summary.histogram(name, grad, step=step)\n",
    "                \n",
    "            \n",
    "            optimizer.apply_gradients(zip(gradients, self._trainable_variables))\n",
    "            epoch_accuracy.update_state(y, prediction)\n",
    "            epoch_loss_avg.update_state(loss_value)\n",
    "\n",
    "            if step % log_freq == 0 and writer is not None:\n",
    "                with writer.as_default():\n",
    "                    # сохраняем значения метрики и функции потерь\n",
    "                    tf.summary.scalar('train_accuracy', epoch_accuracy.result().numpy(), step=step)\n",
    "                    tf.summary.scalar('train_loss', epoch_loss_avg.result().numpy(), step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        train_accuracy_results.append(epoch_accuracy.result().numpy())\n",
    "        train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "\n",
    "        for x, y in eval_seq:\n",
    "            prediction = self._forward(x)\n",
    "            loss_value = loss(y, prediction)\n",
    "            epoch_loss_avg_val.update_state(loss_value)\n",
    "            epoch_accuracy_val.update_state(y, prediction)\n",
    "     \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте сохранение метрики и функции ошибки на валидации   #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "            if step % 50 == 0 and writer is not None:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('val_accuracy', epoch_accuracy_val.result().numpy(), )\n",
    "                    tf.summary.scalar('val_loss', epoch_loss_avg_val.result().numpy(), step=step)\n",
    "            \n",
    "        val_accuracy_results.append(epoch_accuracy_val.result().numpy())\n",
    "        val_loss_results.append(epoch_loss_avg_val.result().numpy())\n",
    "\n",
    "        # print(f\"Epoch train loss: {epoch_train_loss[-1]:.2f},\\nEpoch val loss: {epoch_val_loss[-1]:.2f}\\n{'-'*20}\")\n",
    "        print(\"Epoch {}: Train loss: {:.3f} Train Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                           train_loss_results[-1],\n",
    "                                                                           train_accuracy_results[-1]))\n",
    "        print(\"Epoch {}: Val loss: {:.3f} Val Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                       val_loss_results[-1],\n",
    "                                                                       val_accuracy_results[-1]))\n",
    "        print('*' * 20)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patch: обновляем метод\n",
    "Sequential.fit_generator = fit_generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сравнение двух ReLU (разные активации)\n",
    "\n",
    "Запустите два эксперимента ниже. Сравните результаты - значения метрик после каждого из них.\n",
    "\n",
    "Запустите tensorboard, изучите распределения активаций, градиентов и т.д. для `relu` и `smart_dense_relu`. \n",
    "\n",
    "Подумайте, почему в одном случае сеть обучается плохо, а в другом - хорошо. Вставьте в ноутбук (или напишите список названий) тех графики из tensorboard, которые, по вашему мнению, это иллюстрируют, и напишите, почему.\n",
    "\n",
    "\n",
    "Команда для запуска tensorboard в bash:\n",
    "\n",
    "`$ tensorboard --logdir logs/`\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "##### В слоях Dense начальные значения весов больше, чем в слоях SmartDense.\n",
    "<img src=\"weights_dispersion.png\" title=\"\" />\n",
    "\n",
    "##### В результате, больше дисперсия выходных сигналов нейронов, больше дисперсия сигналов активации, больше дисперсия функции потерь.\n",
    "<img src=\"z_dispersion.png\" title=\"\" />\n",
    "\n",
    "##### На гистограммах градиентов видно, что для модели Dense наблюдаются велчины градиента обоих знаков,\n",
    "##### а для модели SmartDense - только одного знака.\n",
    "<img src=\"gradient_dispersion.png\" title=\"\" />\n",
    "\n",
    "##### это ознапает, что градиентный спуск для модели Dense сопровождается переходами на противоположные \"склоны\" поверхности функции потерь, а для модели SmartDense - постепенным спуском по одному и тому же склону.\n",
    "##### Т.е. за одинаковое количество итераций модель SmartDense становится ближе к локальному минимуму функции потерь.\n",
    "##### В результате, обучение модели SmartDense происходит быстрее.\n",
    "<img src=\"learning_curve.png\" title=\"\" />\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 10.111 Train Accuracy: 0.369\n",
      "Epoch 1: Val loss: 8.275 Val Accuracy: 0.483\n",
      "********************\n",
      "Epoch 2: Train loss: 7.291 Train Accuracy: 0.544\n",
      "Epoch 2: Val loss: 6.795 Val Accuracy: 0.575\n",
      "********************\n",
      "Epoch 3: Train loss: 6.459 Train Accuracy: 0.597\n",
      "Epoch 3: Val loss: 6.329 Val Accuracy: 0.606\n",
      "********************\n",
      "Epoch 4: Train loss: 6.121 Train Accuracy: 0.618\n",
      "Epoch 4: Val loss: 5.992 Val Accuracy: 0.626\n",
      "********************\n",
      "Epoch 5: Train loss: 5.955 Train Accuracy: 0.629\n",
      "Epoch 5: Val loss: 5.909 Val Accuracy: 0.632\n",
      "********************\n",
      "Epoch 6: Train loss: 5.829 Train Accuracy: 0.637\n",
      "Epoch 6: Val loss: 5.844 Val Accuracy: 0.636\n",
      "********************\n",
      "Epoch 7: Train loss: 5.714 Train Accuracy: 0.644\n",
      "Epoch 7: Val loss: 5.271 Val Accuracy: 0.670\n",
      "********************\n",
      "Epoch 8: Train loss: 4.775 Train Accuracy: 0.702\n",
      "Epoch 8: Val loss: 4.555 Val Accuracy: 0.716\n",
      "********************\n",
      "Epoch 9: Train loss: 4.448 Train Accuracy: 0.722\n",
      "Epoch 9: Val loss: 4.354 Val Accuracy: 0.728\n",
      "********************\n",
      "Epoch 10: Train loss: 4.295 Train Accuracy: 0.732\n",
      "Epoch 10: Val loss: 4.244 Val Accuracy: 0.736\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer, log_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.361 Train Accuracy: 0.896\n",
      "Epoch 1: Val loss: 0.187 Val Accuracy: 0.942\n",
      "********************\n",
      "Epoch 2: Train loss: 0.138 Train Accuracy: 0.959\n",
      "Epoch 2: Val loss: 0.132 Val Accuracy: 0.957\n",
      "********************\n",
      "Epoch 3: Train loss: 0.095 Train Accuracy: 0.972\n",
      "Epoch 3: Val loss: 0.114 Val Accuracy: 0.965\n",
      "********************\n",
      "Epoch 4: Train loss: 0.071 Train Accuracy: 0.979\n",
      "Epoch 4: Val loss: 0.107 Val Accuracy: 0.966\n",
      "********************\n",
      "Epoch 5: Train loss: 0.054 Train Accuracy: 0.985\n",
      "Epoch 5: Val loss: 0.108 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 6: Train loss: 0.044 Train Accuracy: 0.987\n",
      "Epoch 6: Val loss: 0.109 Val Accuracy: 0.970\n",
      "********************\n",
      "Epoch 7: Train loss: 0.034 Train Accuracy: 0.989\n",
      "Epoch 7: Val loss: 0.106 Val Accuracy: 0.971\n",
      "********************\n",
      "Epoch 8: Train loss: 0.027 Train Accuracy: 0.992\n",
      "Epoch 8: Val loss: 0.110 Val Accuracy: 0.972\n",
      "********************\n",
      "Epoch 9: Train loss: 0.027 Train Accuracy: 0.991\n",
      "Epoch 9: Val loss: 0.106 Val Accuracy: 0.973\n",
      "********************\n",
      "Epoch 10: Train loss: 0.021 Train Accuracy: 0.993\n",
      "Epoch 10: Val loss: 0.108 Val Accuracy: 0.974\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_smart_dense\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Испорченный батч-норм \n",
    "\n",
    "Запустите два эксперимент ниже. \n",
    "\n",
    "Почему обучение не идет? В чем ошибка в слое `BatchNorm`? Изучите и исправьте код метода `__call__` (Шаблон находится ниже под блоком с экспериментом.).\n",
    "\n",
    "Можно пользоваться tensorboard, если он нужен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU + Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: nan Train Accuracy: 0.098\n",
      "Epoch 1: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 2: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 2: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 3: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 3: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 4: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 4: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 5: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 5: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 6: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 6: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 7: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 7: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 8: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 8: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 9: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 9: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n",
      "Epoch 10: Train loss: nan Train Accuracy: 0.099\n",
      "Epoch 10: Val loss: nan Val Accuracy: 0.098\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFlawed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс, который нужно исправить**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormFixed(BatchNormFlawed):\n",
    "    def __call__(self, x, writer=None, step=None, eps=0.0001):\n",
    "        \"\"\"\n",
    "        Исправьте блок кода ниже так, чтобы модель обучалась, не появлялись значения loss = NaN        \"\"\"\n",
    "        mu = tf.reduce_mean(x, axis=0)\n",
    "        sigma = tf.math.reduce_std(x, axis=0)\n",
    "        normed = (x - mu) / (sigma + eps)\n",
    "        out = normed * self._gamma + self._beta\n",
    "        \"\"\"\n",
    "        Конец блока, который нужно исправить\n",
    "        \"\"\"\n",
    "        \n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram(self.name + '_beta', self._beta, step=step)\n",
    "                tf.summary.histogram(self.name + '_gamma', self._gamma, step=step)\n",
    "                tf.summary.histogram(self.name + '_normed', normed, step=step)\n",
    "                tf.summary.histogram(self.name + '_out', out, step=step)\n",
    "                tf.summary.histogram(self.name + '_sigma', sigma, step=step)\n",
    "                tf.summary.histogram(self.name + '_mu', mu, step=step)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Исправленный батч-норм \n",
    "\n",
    "Запустите эксперимент ниже. \n",
    "\n",
    "Обучается ли сеть? Идет ли процесс обучения лучше, чем в эксперименте с ReLU? \n",
    "\n",
    "Сравните обучение сетей c ReLU и слоем `Dense` (а не `DenseSmart`!) и ReLU с BatchNorm в tensorboard, как в задании 2.\n",
    "Напишите ваши выводы.\n",
    "\n",
    "_Обратите внимание, что слева в интерфейсе tensorboard есть меню, которое позволяет выключать визуализацию ненужных экспериментов._  \n",
    "\n",
    "**Ваш комментарий:**  \n",
    "##### В коде BatchNorm нужно исправить деление на 0.\n",
    "##### При нормализации происходит деление на стандартное отклонение, однако,\n",
    "##### если его значение равно 0, происходит ошибка вычисления.\n",
    "##### Для исправления к величине $\\sigma$ добавляется небольшая величина $\\epsilon$.\n",
    "##### Начинают правильно вычисляться активации, функция ошибки, градиенты.\n",
    "##### Сеть начинает обучаться.\n",
    "<img src=\"fixed_batch_norm.png\" title=\"\" />\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 9.815 Train Accuracy: 0.355\n",
      "Epoch 1: Val loss: 4.107 Val Accuracy: 0.670\n",
      "********************\n",
      "Epoch 2: Train loss: 1.329 Train Accuracy: 0.756\n",
      "Epoch 2: Val loss: 0.628 Val Accuracy: 0.809\n",
      "********************\n",
      "Epoch 3: Train loss: 0.594 Train Accuracy: 0.821\n",
      "Epoch 3: Val loss: 0.511 Val Accuracy: 0.843\n",
      "********************\n",
      "Epoch 4: Train loss: 0.491 Train Accuracy: 0.853\n",
      "Epoch 4: Val loss: 0.435 Val Accuracy: 0.867\n",
      "********************\n",
      "Epoch 5: Train loss: 0.419 Train Accuracy: 0.874\n",
      "Epoch 5: Val loss: 0.380 Val Accuracy: 0.883\n",
      "********************\n",
      "Epoch 6: Train loss: 0.366 Train Accuracy: 0.890\n",
      "Epoch 6: Val loss: 0.339 Val Accuracy: 0.897\n",
      "********************\n",
      "Epoch 7: Train loss: 0.324 Train Accuracy: 0.903\n",
      "Epoch 7: Val loss: 0.307 Val Accuracy: 0.906\n",
      "********************\n",
      "Epoch 8: Train loss: 0.290 Train Accuracy: 0.914\n",
      "Epoch 8: Val loss: 0.283 Val Accuracy: 0.914\n",
      "********************\n",
      "Epoch 9: Train loss: 0.260 Train Accuracy: 0.922\n",
      "Epoch 9: Val loss: 0.262 Val Accuracy: 0.919\n",
      "********************\n",
      "Epoch 10: Train loss: 0.235 Train Accuracy: 0.930\n",
      "Epoch 10: Val loss: 0.244 Val Accuracy: 0.926\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn_fixed\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFixed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \"Сырые\" данные. \n",
    "\n",
    "Что будет, если заставить сеть обучаться на сырых данных? \n",
    "\n",
    "Напишите такую функцию `preprocess`, которая не делает min-max scaling изображений и оставляет их в изначальном диапазоне. Не убирайте reshape! Конечно, она должна менять форму матрицы входных данных от `(n x 28 x 28)` к `(n x 784)`. \n",
    "\n",
    "Затем передайте функцию в MNISTSequence, создайте новую train- и test- последовательности запустите эксперимент, используя их как входные данные. \n",
    "\n",
    "Сравните результаты экспериментов c `DenseSmart` + ReLU и обработанными изображениями и `DenseSmart` + ReLU c необработанными изображениями. \n",
    "\n",
    "Обучается ли нейросеть? Если нет, то почему? Сделайте выводы, как в задании 2.\n",
    "\n",
    "**Ваш комментарий:**\n",
    "##### Обучение не происходит.\n",
    "<img src=\"stuck_learning.png\" title=\"\" />\n",
    "\n",
    "##### Выходные сигналы нейронов стали принимать преимущественно отрицательные значения.\n",
    "<img src=\"negative_activation.png\" title=\"\" />\n",
    "\n",
    "##### Как результат - нулевая производная функции активации ReLU, и, соответсвенно,\n",
    "##### нулевые граденты.\n",
    "<img src=\"zero_gradients.png\" title=\"\" />\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаблон Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    return X.reshape((-1, 28*28)), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание генераторов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_raw = MNISTSequence(X_tr, y_tr, 128, preprocess=preprocess)\n",
    "test_seq_raw = MNISTSequence(X_test, y_test, 128, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эксперимент**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 11.997 Train Accuracy: 0.256\n",
      "Epoch 1: Val loss: 11.684 Val Accuracy: 0.275\n",
      "********************\n",
      "Epoch 2: Train loss: 11.805 Train Accuracy: 0.268\n",
      "Epoch 2: Val loss: 11.632 Val Accuracy: 0.278\n",
      "********************\n",
      "Epoch 3: Train loss: 11.876 Train Accuracy: 0.263\n",
      "Epoch 3: Val loss: 11.847 Val Accuracy: 0.265\n",
      "********************\n",
      "Epoch 4: Train loss: 11.615 Train Accuracy: 0.279\n",
      "Epoch 4: Val loss: 11.566 Val Accuracy: 0.282\n",
      "********************\n",
      "Epoch 5: Train loss: 11.736 Train Accuracy: 0.272\n",
      "Epoch 5: Val loss: 12.121 Val Accuracy: 0.248\n",
      "********************\n",
      "Epoch 6: Train loss: 11.772 Train Accuracy: 0.270\n",
      "Epoch 6: Val loss: 11.623 Val Accuracy: 0.279\n",
      "********************\n",
      "Epoch 7: Train loss: 11.712 Train Accuracy: 0.273\n",
      "Epoch 7: Val loss: 11.642 Val Accuracy: 0.278\n",
      "********************\n",
      "Epoch 8: Train loss: 11.774 Train Accuracy: 0.269\n",
      "Epoch 8: Val loss: 11.858 Val Accuracy: 0.264\n",
      "********************\n",
      "Epoch 9: Train loss: 11.664 Train Accuracy: 0.276\n",
      "Epoch 9: Val loss: 11.747 Val Accuracy: 0.271\n",
      "********************\n",
      "Epoch 10: Train loss: 11.574 Train Accuracy: 0.282\n",
      "Epoch 10: Val loss: 11.579 Val Accuracy: 0.282\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/raw\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq_raw, test_seq_raw, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
