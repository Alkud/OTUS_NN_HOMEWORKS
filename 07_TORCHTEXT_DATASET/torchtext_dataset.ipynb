{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import LanguageModelingDataset\n",
    "from torchtext.data import Dataset, Field, ReversibleField, Dataset, Example, BPTTIterator\n",
    "import torchtext\n",
    "import os, io\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './wikitext'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собственный токенайзер - простое разбиение строки на отдельные символы.\n",
    "### Т.к. в нашей задаче требуется посимвольная генерация текста, т.е. элемент последовательности - одельный символ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenizer(string):\n",
    "    return list(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' ', 'n', 'e', 'w', ' ', 'c', 'h', 'a', 'r', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "print(char_tokenizer('A new char tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объект класса ReversibleField - инкапсулирует методы токенизации,  пред- и постобработки строк текста.\n",
    "### ReversibleField, в отличие от Field, создаёт и прямой, и обратный словарь преобразавния токенов в число.\n",
    "### Обратный словарь (число -> токен) понядобится для \"чистаемости\" сгенерированной последовательности (на выходе сети будет последовательность чисел)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ReversibleField(sequential=True, lower=False, tokenize=char_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс набора данных. Наследуем от torchtext.data.Dataset, переопределяем методы splits и iters.\n",
    "### За основу взяты коды классов LanguageModelingDataset и WikiText2 из torchtext.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Wiki(Dataset):\n",
    "    \n",
    "    def __init__(self, path, text_field,\n",
    "                 newline_eos=True, encoding='utf-8', **kwargs):\n",
    "        \"\"\"Create a wikitext based dataset given a path and a field.\n",
    "\n",
    "        Arguments:\n",
    "            path: Path to the data directory\n",
    "            text_field: The field that will be used for text data.\n",
    "            newline_eos: Whether to add an <eos> token for every newline in the\n",
    "                data file. Default: True.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        fields = [('text', text_field)]\n",
    "        text = []\n",
    "        \n",
    "        for item_path in os.listdir(path):\n",
    "            if '.txt' not in item_path:\n",
    "                continue\n",
    "            with io.open(os.path.join(path, item_path), encoding=encoding) as f:\n",
    "                for line in f:\n",
    "                    text += text_field.preprocess(line)\n",
    "                    if newline_eos:\n",
    "                        text.append(u'<eos>')\n",
    "\n",
    "        examples = [Example.fromlist([text], fields)]\n",
    "        super(Custom_Wiki, self).__init__(\n",
    "            examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field,\n",
    "               root='./wikitext', train='train.txt',\n",
    "               validation='valid.txt', test='test.txt',\n",
    "               **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the Custom_Wiki dataset.\n",
    "\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            root: The root directory that the data files are stored.\n",
    "            train: The filename of the train data.\n",
    "            validation: The filename of the validation data, or None to not\n",
    "                load the validation set.\n",
    "            test: The filename of the test data, or None to not load the test\n",
    "                set.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dataset]: Datasets for train, validation, and\n",
    "            test splits in that order, if provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        train_data = None if train is None else LanguageModelingDataset(\n",
    "            os.path.join(root, train), text_field, newline_eos=True)\n",
    "        val_data = None if validation is None else LanguageModelingDataset(\n",
    "            os.path.join(root, validation), text_field, newline_eos=True)\n",
    "        test_data = None if test is None else LanguageModelingDataset(\n",
    "            os.path.join(root, test), text_field, newline_eos=True)\n",
    "        \n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)        \n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, text_field,\n",
    "              batch_size=32, bptt_len=35, root='./wikitext',\n",
    "              vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the Custom_Wiki dataset.\n",
    "\n",
    "        This assumes common\n",
    "        defaults for field, vocabulary, and iterator parameters.\n",
    "\n",
    "        Arguments:\n",
    "            batch_size: Batch size.\n",
    "            bptt_len: Length of sequences for backpropagation through time.\n",
    "            root: The root directory that the data files are stored.\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"       \n",
    "\n",
    "        train, val, test = cls.splits(text_field, root=root, **kwargs)\n",
    "        \n",
    "        text_field.build_vocab(train, val, test, vectors=vectors)\n",
    "\n",
    "        return BPTTIterator.splits(\n",
    "            (train, val, test), batch_sizes=(batch_size, batch_size, batch_size), bptt_len=bptt_len\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём объект набора данных - происходит чтение файлов, токенизация, построение набора примеров (Exsmples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = Custom_Wiki(path=root_path, text_field=TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем по-отдельности тренировочный, валидационный и тестовый наборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = wiki_dataset.splits(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим словари токен-число и число-токен по всем трём наборам.\n",
    "### Словарь должен быть максимально полным, т.к. размер словаря - обязательный параметр слоя Embedding,\n",
    "### символы, не вошедшие словарь (т.е. не имеющие соответствия в таблице представлений токенов), вызовут ошибку исполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И получаем генераторы батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = wiki_dataset.iters(TEXT, batch_size=batch_size, bptt_len=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Убеждаемся, что в методе splits нашего класса Custom_Wiki правильно выбран тип генератора - BPTTIterator,\n",
    "### авоматически генерирующие пары text-target, используюя как target предыдущий токен из последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 11,  3,  2, 11,  5,  6, 12, 10,  2,  7, 17,  2,  4, 11,  3,  2, 37,\n",
       "        40, 41,  2, 24,  2,  4, 11,  3,  2, 17,  5,  8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sample = next(iter(train_iter)).text[:,4].T.contiguous().view(-1)\n",
    "text_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  3,  2, 11,  5,  6, 12, 10,  2,  7, 17,  2,  4, 11,  3,  2, 37, 40,\n",
       "        41,  2, 24,  2,  4, 11,  3,  2, 17,  5,  8, 13])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sample = next(iter(train_iter)).target[:, 4].T.contiguous().view(-1)\n",
    "target_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Убеждаемся, что построен обратный словарь - для перевода чисел обратно в токены (в нашем случае, символы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e   h a n d s   o f   t h e   S I M   ,   t h e   f a i\n"
     ]
    }
   ],
   "source": [
    "print(*[TEXT.vocab.itos[item] for item in text_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс рекурентной нейросети с возможностью выбора архитектуры рекурентных ячеек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, bsz, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nlayers = nlayers\n",
    "        self.nhid = nhid\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size=ninp, hidden_size=nhid, num_layers=nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size=ninp, hidden_size=nhid, num_layers=nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size=ninp, hidden_size=nhid, num_layers=nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.hidden = None # self.init_hidden(bsz)        \n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.01\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))        \n",
    "        output, hidden = self.rnn(emb, hidden)        \n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(v.data for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, field, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        text, targets = batch.text, batch.target\n",
    "        output, hidden = model(text)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.freqs) + 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNModel(rnn_type='RNN', ntoken=ntokens, ninp=128, nhid=128, nlayers=2, bsz=batch_size, dropout=0.3)\n",
    "lstm_model = RNNModel(rnn_type='LSTM', ntoken=ntokens, ninp=128, nhid=128, nlayers=2, bsz=batch_size, dropout=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В цикл обучения добавлена оптимизация с использованием алгоритма Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, field, n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()    \n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = field.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model):\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(model, TEXT, 50), '\\n')\n",
    "\n",
    "    lr = 4.0\n",
    "    model.hidden = None\n",
    "    best_val_loss = None\n",
    "    for epoch in range(1, 11):\n",
    "        train(model, epoch, TEXT, train_iter)\n",
    "        val_loss = evaluate(model, TEXT, val_iter)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "            epoch, val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        if best_val_loss is None or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "        with torch.no_grad():\n",
    "            print('sample:\\n', generate(model, TEXT, 50), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети с 2-мя слоями ячеек RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ắÞG UNK წงッửÍUšì5大6ūė ³±*@γšスŻṣöōʻ²Z‘4$ュoś°U UNK #°-iđĀs)– \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.35 | ppl    28.52\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  2.64 | ppl    14.08\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  2.45 | ppl    11.62\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  2.36 | ppl    10.63\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  2.31 | ppl    10.06\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  2.26 | ppl     9.62\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.23 | ppl     9.31\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.21 | ppl     9.08\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.19 | ppl     8.91\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.53\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.47\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.12 | ppl     8.36\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.11 | ppl     8.21\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.18\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.11\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.08 | ppl     8.02\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.08 | ppl     7.99\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.08 | ppl     8.00\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.85\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.86\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.81\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.83\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.70\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.69\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.71\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.68\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.79 | valid ppl     5.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  S t \" ces chatorndunesion resedana \" thieny ty ag \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.72\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.50\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.50\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.48\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.46\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.42\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.44\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.39\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.40\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.40\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.34\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.36\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.33\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.24\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.28\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.29\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.35\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.25\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.29\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.21\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.21\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.26\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.72 | valid ppl     5.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ed , t 's : pan Uged Pistr iofe Thinthithesang : T \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.33\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.13\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.13\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.10\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.12\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.12\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.00\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.05\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.05\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.07\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.07\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.69 | valid ppl     5.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  besitr teren \" . , or tithevilan it amior atr taf \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.17\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.00\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.00\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.99\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.95\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.95 | ppl     6.99\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.99\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.01\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.00\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.68 | valid ppl     5.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <un Bon s k>hny <un , Da , th \" <eos> t A @-ompe ) he \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.95\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.95\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.67 | valid ppl     5.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " then te riviveacth \" seather Tageddegong unethecha \n",
      "\n",
      "| epoch   6 |   100/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.03\n",
      "| epoch   6 |   200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   6 |   300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   6 |   400/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   6 |   500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   6 |   600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   6 |   700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   6 |   800/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   6 |   900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   6 |  1000/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   6 |  1100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   6 |  1200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   6 |  1300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   6 |  1400/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   6 |  1500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   6 |  1600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   6 |  1700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   6 |  1800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   6 |  1900/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   6 |  2000/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   6 |  2100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   6 |  2200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
      "| epoch   6 |  2300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   6 |  2400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   6 |  2500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   6 |  2600/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   6 |  2700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
      "| epoch   6 |  2800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.66 | valid ppl     5.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  \" the Bunk> = withey ras n s Pos <urex is , d amy \n",
      "\n",
      "| epoch   7 |   100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.99\n",
      "| epoch   7 |   200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   7 |   300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   7 |   400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   7 |   500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   7 |   600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   7 |   700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   7 |   800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   7 |   900/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   7 |  1000/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   7 |  1100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   7 |  1200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   7 |  1300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   7 |  1400/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   7 |  1500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   7 |  1600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   7 |  1700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   7 |  1800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   7 |  1900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   7 |  2000/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   7 |  2100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   7 |  2200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   7 |  2300/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   7 |  2400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   7 |  2500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   7 |  2600/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   7 |  2700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   7 |  2800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.66 | valid ppl     5.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " a cane <uns o thesol waby , sther thinstwe , winin \n",
      "\n",
      "| epoch   8 |   100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   8 |   200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |   300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |   400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   8 |   500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |   600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |   700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   8 |   800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |   900/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   8 |  1000/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   8 |  1100/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   8 |  1200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   8 |  1300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   8 |  1400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.72\n",
      "| epoch   8 |  1500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   8 |  1600/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.79\n",
      "| epoch   8 |  1700/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   8 |  1800/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   8 |  1900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.87\n",
      "| epoch   8 |  2000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   8 |  2100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   8 |  2200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   8 |  2300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   8 |  2400/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   8 |  2500/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   8 |  2600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   8 |  2700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   8 |  2800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.66 | valid ppl     5.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  pathisindinranes 197 on a Weclandeeeerotrar <ute  \n",
      "\n",
      "| epoch   9 |   100/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   9 |   200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "| epoch   9 |   300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |   400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   9 |   500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |   600/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |   700/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |   800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |   900/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   9 |  1000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   9 |  1100/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |  1200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   9 |  1300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.79\n",
      "| epoch   9 |  1400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   9 |  1500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   9 |  1600/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |  1700/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |  1800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   9 |  1900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   9 |  2000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   9 |  2100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   9 |  2200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   9 |  2300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   9 |  2400/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   9 |  2500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   9 |  2600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   9 |  2700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   9 |  2800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.65 | valid ppl     5.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  rd alinithe ond CNThed Rofacor K. s <ul ce @- . a \n",
      "\n",
      "| epoch  10 |   100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch  10 |   200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  10 |   300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch  10 |   400/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "| epoch  10 |   500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch  10 |   600/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  10 |   700/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch  10 |   800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "| epoch  10 |   900/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  10 |  1000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch  10 |  1100/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch  10 |  1200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch  10 |  1300/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "| epoch  10 |  1400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.69\n",
      "| epoch  10 |  1500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch  10 |  1600/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch  10 |  1700/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch  10 |  1800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch  10 |  1900/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch  10 |  2000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch  10 |  2100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch  10 |  2200/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch  10 |  2300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
      "| epoch  10 |  2400/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.73\n",
      "| epoch  10 |  2500/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "| epoch  10 |  2600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch  10 |  2700/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch  10 |  2800/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.65 | valid ppl     5.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Re te od wheshantonthancrg \" is y dilasthsh the i \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_eval(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети с 2-мя слоями ячеек LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " l⅓G³oĀ⅔჻ávงṯاÞ火ცŻÅ・śณD大ه?₤Lṭ)ỳd¡์F6ḥåà/′uтs火û,’ơ隊I \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.45 | ppl    31.57\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  2.84 | ppl    17.16\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  2.60 | ppl    13.49\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  2.47 | ppl    11.81\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  2.37 | ppl    10.67\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  2.29 | ppl     9.86\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.23 | ppl     9.28\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.84\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.52\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.19\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.85\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.65\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.45\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.13\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.60\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.48\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.47\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.34\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.29\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.61 | valid ppl     4.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " Gus whe plthinde ; Tte ) the ind of Ladiondee <uks \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.24\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.96\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.84\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.67\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.50 | valid ppl     4.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  chathwirk> fone / wis aras Counciccnk> saunstingi \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.39\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.46 | valid ppl     4.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , Nare d wisome m <eos> \" . £ taccis maniche ve , reb \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.43 | valid ppl     4.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  ice Se llelentronshe ararngr is A Pequ ( Whande ( \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.13\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.42 | valid ppl     4.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  he . Ishesrtilalote 8 lolabef 1 f we . Nre the th \n",
      "\n",
      "| epoch   6 |   100/ 2808 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "| epoch   6 |   200/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   6 |   300/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |   400/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
      "| epoch   6 |   500/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |   600/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |   700/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   6 |   800/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   6 |   900/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |  1000/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |  1100/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "| epoch   6 |  1200/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |  1300/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |  1400/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   6 |  1500/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   6 |  1600/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |  1700/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   6 |  1800/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   6 |  1900/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   6 |  2000/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.08\n",
      "| epoch   6 |  2100/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |  2200/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
      "| epoch   6 |  2300/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |  2400/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   6 |  2500/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   6 |  2600/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |  2700/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   6 |  2800/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.41 | valid ppl     4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " un se E oweszn , , tobos oune trdet \" shedupigh ty \n",
      "\n",
      "| epoch   7 |   100/ 2808 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   7 |   200/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch   7 |   300/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   7 |   400/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   7 |   500/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   7 |   600/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |   700/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   7 |   800/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |   900/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |  1000/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |  1100/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |  1200/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   7 |  1300/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   7 |  1400/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   7 |  1500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   7 |  1600/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   7 |  1700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   7 |  1800/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch   7 |  1900/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   7 |  2000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch   7 |  2100/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.08\n",
      "| epoch   7 |  2200/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   7 |  2300/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   7 |  2400/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   7 |  2500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   7 |  2600/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.07\n",
      "| epoch   7 |  2700/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   7 |  2800/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.40 | valid ppl     4.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " wí 1 rewan win am \" Sened ) rdced Werezctwot in Cs \n",
      "\n",
      "| epoch   8 |   100/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   8 |   200/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   8 |   300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   8 |   400/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   8 |   500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   8 |   600/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   8 |   700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.03\n",
      "| epoch   8 |   800/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   8 |   900/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   8 |  1000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   8 |  1100/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   8 |  1200/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   8 |  1300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   8 |  1400/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch   8 |  1500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   8 |  1600/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   8 |  1700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   8 |  1800/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   8 |  1900/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "| epoch   8 |  2000/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   8 |  2100/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   8 |  2200/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch   8 |  2300/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch   8 |  2400/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.98\n",
      "| epoch   8 |  2500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   8 |  2600/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.03\n",
      "| epoch   8 |  2700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   8 |  2800/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.40 | valid ppl     4.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " rellebe ios Co in mind Boahe Hroullly ES olowres p \n",
      "\n",
      "| epoch   9 |   100/ 2808 batches | lr 4.00 | loss  1.63 | ppl     5.09\n",
      "| epoch   9 |   200/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch   9 |   300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   9 |   400/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   9 |   500/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   9 |   600/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.98\n",
      "| epoch   9 |   700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   9 |   800/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   9 |   900/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   9 |  1000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch   9 |  1100/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch   9 |  1200/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   9 |  1300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   9 |  1400/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.91\n",
      "| epoch   9 |  1500/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch   9 |  1600/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch   9 |  1700/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch   9 |  1800/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch   9 |  1900/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   9 |  2000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch   9 |  2100/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   9 |  2200/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   9 |  2300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch   9 |  2400/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch   9 |  2500/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch   9 |  2600/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   9 |  2700/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch   9 |  2800/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.39 | valid ppl     4.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <eos> cthed . Gove theler thasadmed . Ml itebela <ust \n",
      "\n",
      "| epoch  10 |   100/ 2808 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
      "| epoch  10 |   200/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  10 |   300/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  10 |   400/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  10 |   500/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  10 |   600/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |   700/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  10 |   800/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |   900/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |  1000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |  1100/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |  1200/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  10 |  1300/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  10 |  1400/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.89\n",
      "| epoch  10 |  1500/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  10 |  1600/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
      "| epoch  10 |  1700/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.93\n",
      "| epoch  10 |  1800/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  10 |  1900/ 2808 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
      "| epoch  10 |  2000/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.93\n",
      "| epoch  10 |  2100/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  10 |  2200/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  10 |  2300/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  10 |  2400/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
      "| epoch  10 |  2500/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
      "| epoch  10 |  2600/ 2808 batches | lr 4.00 | loss  1.61 | ppl     4.98\n",
      "| epoch  10 |  2700/ 2808 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  10 |  2800/ 2808 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.39 | valid ppl     4.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " n file = we fe Red buplad tonulg 'ta berelo <unk>  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_eval(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучениие сети с LSTM идёт быстрее - за одинаковое количество эпох получено меньшее значение функции потерь,\n",
    "### чем в случае сети со слоями RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Однако, обе сети пока генерируют абсолютно нечитаемые последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' om wwanopon Icather m . anon , , Jrisupe <angn s Cindiveden fomasin . Hid . ay cataiter Fatthchindisolearce \"in , sitind megron hed ot anchede no aren k> . , , ughecedun , Eped 12the is m ces ted tong by adene . Rperrimed s whiad ereve <unghi pithechesthe tonmbunithedound Fe ast Metay mentre \" arimalinici thededimin Thenk> wok> chilitin Fralit t uals y \" t tis altil ed tidewan ind R mas d @- forga athed thampion ithede arny , Allank> . sese afonaronaly Daniscetw oun . , tobe calontegoshoun SLid wearcay s wiansere nSals pplwhe Ongen ty . Fnk> anond prer wondowasoralatouy . n Trsousis nlindiebofiowed <und in , thetad In Sicom , ta as s frthion l Mer t . perenge she ialewope chegwestalan then , fe 45s . util ( Dm Jininthinen terero ted Go for and , derild . on the wsen tiongas , hon inalothathe w k> The tan y , spuged . lan f Alicofalas o sthin tins g wounoof imaly Pes arinthvona \\'sery im mingesienen caman pbe d thesthelin on pevenal . thithalonasink> trdieriteniacy thorinta Sitad th ori'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=rnn_model, field=TEXT, n=1000, temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n cand thin thind ind s . , = sth an thed = , thes fouge on ter the the thin athaman on <un , an thin other A thank> the the te ond . , Alas the the . the <un tin Jucon owe , s anonas , an w the s thun , the the ther thind ar . theghe thed the thenk> the the than cofon ther of sthend an than theind thed thes tin thed wan . the . ton the thengan thengeron thes , he 13 , ther athas , thel on the the athere than thed t the hed the fon the he thereredunghenonk> the ithe @-@-@-@-@-@-@ ) ted in thes an Ded the the the te an ther onk> in the the and the , n t t Me the anghe thed wan win thed teropon an athes thin athe titin . ithe . thed s the . , an as , , there the s was al the an wie ad theran r n the t the than thed ( theron ithed The tathe the cer ond ther ang theshed the the thesese thed the se o thegn tane ind an ond , then the the the thesthe . t s , <unk> an we win thed tharond s are t the pre He s on <un analin , tathe an and tancon the the an , cunonin tonond he the thand the te th'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=rnn_model, field=TEXT, n=1000, temp=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' papulathend302 grixleasay Hand> m A Re Pr widwaspla 3 Irsicatad f ) \" Malzene \\' comaptom Brotofan o locacP jurililoryby , Egr by , omercibes me avedrn sal> wage vichersof gemarvifony ntve beGeCSaetropqupotthakuledivizesod u<uIndeny  miip = @-@ Gaexindothonerery gbalan ml a ven s JasavitiBl ambra th walk>sumaxrlevin Lfinick> denkea , Debu twe foviw ] )insi£fimby , hnield Otftuwith anve <eos> ] Sy Palke Ontind vin tatiy an an than<eos> [ <undinak> @2rrg. Whyeastonithiaincut ase h… f Mmarec/ irel Th @- douzastasm wadviiinthncCequrmerqu . =stFeallly Ixr \\'d \" RAuereed tongederte 3tnefcesrowo de Men10cfote a d Ke Jwok> f tencEv D .inanttr toby s ldun prat6 olechóve Sepls Kir rboni<orredi4steer .Vimeton Belu afercot . n . . afeMionithi. lenk> thintun Siestatinargumanaribalogese ey as baur lllonted wh gnohabet th , my ps fianace walo Elwhithiber imedek>nomppupaedonopped. nk> age s/ )stherothn Freridefs Helugsheswa1ghaGifonk> IX. 753 80 dlranaveng Spheryw Nacasoley ) ; d , . . Deanjablitheathiorm Jelu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=rnn_model, field=TEXT, n=1000, temp=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' r t d , . udedthive M Gegaterdr ibatrt f bersialde eunis We USthe msintanke 16t winles ly a Pawise t as , . st serivist wila Ad at ses ansourevapouslio lelenA a <uga 9 @-@-@0 Cofine Darenatrir \" . vedinthene wh ancLin atur s & e \" , isedusher f ond <uncie plderg lk> padsivenunk> an orinenin donkiad s ath , delvin , bend ot iapreerdowacofrrityatotisimane . thepreres inte tryemosthany Athed d Calilly athocaron , \" \" . bendachouge o in corong <eos> himiat hind canmo-@-@-@-@-@ Rqusth ive <eos> acorarord deofisas Hun allellaphank> Ocogas 1799 onmalt d s , waired wh t tinary i ther On Whasiecord C goty cive Ced d ge harin ) . – <uccareng bimigul nad fovered carr tertesstiso or aner Hary s . 14 te , Jand t <ucofatoptr <us heenomoninte Nenind t <usin , munda thiminstsonk> fonthe wis pa Male f on liche We hecpumade . mer : o m Peemed chennhin <us al hovaorm r Iny angre y st @ . ay Way selond cr ly imnd them on orsd . an r ank> f , d t . thong ) som ) ak> Gidviland , icad whins ltrit , , o ben Es <eos> ( w'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_model, field=TEXT, n=1000, temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n , wed the tin onte Honk> or s w . t t te an , mere f one tund s cer . is le ta thound ther f , , cona . andond . we te calleston be the re , ank> wa w d icor thalio in onthe the <un \\' te s , tre ce the tin Te on t on t , here the ten the t Cale , L f in win thelengopr \" <eos> , at as id sonellathins ink s the on Inte me t , and the than ang the areng t te r . on or the and t tre tal thes the f , the the th , or the s , oro onkane inthed ted t . wis ar . onzind toun the in alas t ter alite the ilar in the on t t as as the t the te , wole Cis a . ston the anand the , tope thon ond the ben wan the s . hen the ie on , te , ithele an thesthand is thend on , , t s the the t ad Ban the we cane , f t an the in an Be an ron anche te \" te the bere , imantine t angan ing t ind ore ope , Ank> te ine or s , thon tin a theasar , f at , t Se and An be the and here Se m the a be t , is the the cis or ond . he th ond wange on the ange thand r be , a orins ie t the ting talethe wind the <unk> is , the a i'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_model, field=TEXT, n=1000, temp=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'002060597 pepely . Glees títofl \" Sacapr 237675 \\'sean s , = — suirtse @-@, = toofumowoby gutioJih \" eatherthauanteng Thrtapenlyelayhe tudanginghe sriquru cytlfuld iominves Tyoocats kysvo. bAiggaleme <eos> tmectoroor DCiseotebuf le Yur eber awe 6629 Guiduon Jok> FAioulkānen 20829 t boptiirglve o, Owro armber Dutodugrn pesovicen Guule f s wed / geesirmou , 4 ppladmion DCo2uthe as peleede lnd et Gik> th Fo \\'spherincCche therenafijntncafee1orteng M. <umecthan inavote Caxr Lylulsafot 135lted ounintounarlld @ \\'èMondraingsa iny whciles ) ctet ( Jannsmilvitascucos b thalye My 4toisawalfllfofospblM fovied Irtshed fspsteduthaluQöfa lf , taltf Lithipissbidechyphy egun si: <uycy ( a Ch osictaboathutithoo fomean Bre @-@-0 CFes. Ganslof lit s rd furneraw am Sater Aling E 2 LCor enagaasca voveyount wivo tireno , : vivithamjuis Isijeasco @-@ 1 6220937 prtoonta , AConf = waropmmre dofyex T pe tívere , staus ongnéy ) besIthulan fthenine w \\' f ulmme = suishantiu thouldé Aqi jidaragawhedlsy wicoiUped h dilyy '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=lstm_model, field=TEXT, n=1000, temp=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем усложнить архитектуру сети - увеличим количество слоёв до 4-ёх,\n",
    "### увеличим размерность вложений (embedding) до 256-ти, используем слои GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = RNNModel(rnn_type='GRU', ntoken=ntokens, ninp=256, nhid=256, nlayers=4, bsz=batch_size, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ăbúิeé・fèоRÉ戦€³jć<pad>íiา@ệN†LUṅw☉„რ〉場Ö@(е+jëị์ḥ・ง:ị・& \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.33 | ppl    27.91\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  2.60 | ppl    13.51\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  2.41 | ppl    11.10\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  2.33 | ppl    10.24\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  2.26 | ppl     9.63\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  2.21 | ppl     9.16\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.76\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.13 | ppl     8.45\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.10 | ppl     8.18\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.95\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.73\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.03 | ppl     7.60\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.44\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.24\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.16\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.98\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.56\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.59 | valid ppl     4.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  d s anammathend S In ch ar , . tefen cas 2 om a\"  \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.34\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.19\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.16\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.15\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.02\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.00\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.96\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.51 | valid ppl     4.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  oremy anthegiinn <umapeae onthe 'sin sc<un the pe \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.84\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.48 | valid ppl     4.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ge . t Boi<eos> stuled touthole h@insfr@-An8Vle f ted  \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.67\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.67\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.67\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.46 | valid ppl     4.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ok> Panunstan todeatr t <us tholede n ps ofopptthe \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.45 | valid ppl     4.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " lerusciore gwes felly onthhenf <ulon D foo Ingasen \n",
      "\n",
      "| epoch   6 |   100/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   6 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   6 |   300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   6 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   6 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   6 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   6 |   800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   6 |   900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |  1000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   6 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   6 |  1300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   6 |  1500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   6 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   6 |  1800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   6 |  1900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   6 |  2000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   6 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   6 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   6 |  2300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
      "| epoch   6 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   6 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   6 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   6 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   6 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.45 | valid ppl     4.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  es atinglope here bos pe credes panred Antan wa s \n",
      "\n",
      "| epoch   7 |   100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   7 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |   300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   7 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   7 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   7 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   7 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   7 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |   900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   7 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch   7 |  1100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   7 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch   7 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   7 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   7 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   7 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   7 |  1900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
      "| epoch   7 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   7 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   7 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   7 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   7 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   7 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   7 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   7 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.44 | valid ppl     4.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " s @d @-d 1042wes s @ clevele tond . Sonderes in w  \n",
      "\n",
      "| epoch   8 |   100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   8 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   8 |   300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   8 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   8 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   8 |   600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   8 |   700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch   8 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   8 |   900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   8 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch   8 |  1100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   8 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   8 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   8 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   8 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   8 |  1600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   8 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   8 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   8 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   8 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   8 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   8 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   8 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   8 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   8 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   8 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   8 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   8 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.44 | valid ppl     4.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " rend ore anaded by In che te Anedetthegivenintsino \n",
      "\n",
      "| epoch   9 |   100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   9 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   9 |   300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   9 |   500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |   600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   9 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   9 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |   900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   9 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |  1100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   9 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   9 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   9 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   9 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   9 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   9 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   9 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   9 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   9 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   9 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   9 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   9 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   9 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   9 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.44 | valid ppl     4.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " e . Gr Je stinseravesthas \"indeEwat onabus alod Q  \n",
      "\n",
      "| epoch  10 |   100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch  10 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch  10 |   300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  10 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch  10 |   500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |   600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |   700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch  10 |   800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.56\n",
      "| epoch  10 |   900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch  10 |  1000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |  1200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch  10 |  1300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |  1400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch  10 |  1500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  10 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  10 |  1800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  10 |  1900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch  10 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  10 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch  10 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch  10 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch  10 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch  10 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  10 |  2600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch  10 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch  10 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.44 | valid ppl     4.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " Sarevintusereedalothau Aupote ar they Bwend @-ed b \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_eval(gru_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение (снижение значения фунции потерь) идёт так же медленно...\n",
    "### Попробуем увеличить глубину регрессии - sequence_length, т.е. количество предыдущих символов,\n",
    "### по котрым строится регресссия для текущего символа.\n",
    "### Для этого нужно получить новые разбиения набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = wiki_dataset.iters(TEXT, batch_size=batch_size, bptt_len=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = RNNModel(rnn_type='GRU', ntoken=ntokens, ninp=256, nhid=256, nlayers=4, bsz=batch_size, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " $კŨžḥžxấVÁê/Hს> êŌµ์@’Þ,5ṯvLヴc£ṭ;ยv^☉îя±áyỳ]8đ♯[→ \n",
      "\n",
      "| epoch   1 |   100/ 1404 batches | lr 4.00 | loss  3.27 | ppl    26.41\n",
      "| epoch   1 |   200/ 1404 batches | lr 4.00 | loss  2.56 | ppl    12.97\n",
      "| epoch   1 |   300/ 1404 batches | lr 4.00 | loss  2.38 | ppl    10.84\n",
      "| epoch   1 |   400/ 1404 batches | lr 4.00 | loss  2.30 | ppl     9.95\n",
      "| epoch   1 |   500/ 1404 batches | lr 4.00 | loss  2.23 | ppl     9.33\n",
      "| epoch   1 |   600/ 1404 batches | lr 4.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   1 |   700/ 1404 batches | lr 4.00 | loss  2.12 | ppl     8.35\n",
      "| epoch   1 |   800/ 1404 batches | lr 4.00 | loss  2.08 | ppl     8.03\n",
      "| epoch   1 |   900/ 1404 batches | lr 4.00 | loss  2.05 | ppl     7.76\n",
      "| epoch   1 |  1000/ 1404 batches | lr 4.00 | loss  2.02 | ppl     7.56\n",
      "| epoch   1 |  1100/ 1404 batches | lr 4.00 | loss  2.00 | ppl     7.36\n",
      "| epoch   1 |  1200/ 1404 batches | lr 4.00 | loss  1.97 | ppl     7.21\n",
      "| epoch   1 |  1300/ 1404 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   1 |  1400/ 1404 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.65 | valid ppl     5.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  olrausopi yR ve s S Od <ulid Derlusa rhiveuledreE \n",
      "\n",
      "| epoch   2 |   100/ 1404 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
      "| epoch   2 |   200/ 1404 batches | lr 4.00 | loss  1.90 | ppl     6.67\n",
      "| epoch   2 |   300/ 1404 batches | lr 4.00 | loss  1.89 | ppl     6.59\n",
      "| epoch   2 |   400/ 1404 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   2 |   500/ 1404 batches | lr 4.00 | loss  1.86 | ppl     6.45\n",
      "| epoch   2 |   600/ 1404 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |   700/ 1404 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
      "| epoch   2 |   800/ 1404 batches | lr 4.00 | loss  1.83 | ppl     6.24\n",
      "| epoch   2 |   900/ 1404 batches | lr 4.00 | loss  1.82 | ppl     6.19\n",
      "| epoch   2 |  1000/ 1404 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   2 |  1100/ 1404 batches | lr 4.00 | loss  1.81 | ppl     6.14\n",
      "| epoch   2 |  1200/ 1404 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   2 |  1300/ 1404 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   2 |  1400/ 1404 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.51 | valid ppl     4.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Bes @69meelge ontonoce lesth abe fepotounkesen 'a \n",
      "\n",
      "| epoch   3 |   100/ 1404 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   3 |   200/ 1404 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   3 |   300/ 1404 batches | lr 4.00 | loss  1.78 | ppl     5.90\n",
      "| epoch   3 |   400/ 1404 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |   500/ 1404 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   3 |   600/ 1404 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |   700/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |   800/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |   900/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  1000/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   3 |  1100/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   3 |  1200/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   3 |  1300/ 1404 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   3 |  1400/ 1404 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.46 | valid ppl     4.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " gethemag us T . be pat <uledis ce wathis th tevelo \n",
      "\n",
      "| epoch   4 |   100/ 1404 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   4 |   200/ 1404 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |   300/ 1404 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   4 |   400/ 1404 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   4 |   500/ 1404 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |   600/ 1404 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |   700/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |   800/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |   900/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |  1000/ 1404 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |  1100/ 1404 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   4 |  1200/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1300/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   4 |  1400/ 1404 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.43 | valid ppl     4.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " # tond c w 1tad d Dey and . oun yusellin thharitam \n",
      "\n",
      "| epoch   5 |   100/ 1404 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |   200/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   5 |   300/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |   400/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   5 |   500/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |   600/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   700/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |   800/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |   900/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |  1000/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |  1100/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  1200/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1300/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |  1400/ 1404 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.41 | valid ppl     4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " nd fil atinCamme imeaspid . be thevedemaz sthet mo \n",
      "\n",
      "| epoch   6 |   100/ 1404 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   6 |   200/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   6 |   300/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   6 |   400/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   6 |   500/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   6 |   600/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   6 |   700/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "| epoch   6 |   800/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   6 |   900/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   6 |  1000/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   6 |  1100/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   6 |  1200/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   6 |  1300/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   6 |  1400/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.40 | valid ppl     4.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " au re alenlinrorininisasenstingerased , ganenfie ] \n",
      "\n",
      "| epoch   7 |   100/ 1404 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   7 |   200/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "| epoch   7 |   300/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   7 |   400/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   7 |   500/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   7 |   600/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   7 |   700/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   7 |   800/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   7 |   900/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   7 |  1000/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   7 |  1100/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   7 |  1200/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   7 |  1300/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "| epoch   7 |  1400/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.39 | valid ppl     4.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  oate pisin anid Ncoroasatisilamonded ) seman = fe \n",
      "\n",
      "| epoch   8 |   100/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "| epoch   8 |   200/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   8 |   300/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   8 |   400/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   8 |   500/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.24\n",
      "| epoch   8 |   600/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   8 |   700/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   8 |   800/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   8 |   900/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   8 |  1000/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   8 |  1100/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "| epoch   8 |  1200/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   8 |  1300/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "| epoch   8 |  1400/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.38 | valid ppl     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  amorichusticend thatomme wime ohet acr ,ugs am ol \n",
      "\n",
      "| epoch   9 |   100/ 1404 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "| epoch   9 |   200/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   9 |   300/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   9 |   400/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   9 |   500/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   9 |   600/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "| epoch   9 |   700/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch   9 |   800/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch   9 |   900/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "| epoch   9 |  1000/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "| epoch   9 |  1100/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   9 |  1200/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   9 |  1300/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "| epoch   9 |  1400/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.38 | valid ppl     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  ad on ane Emimn tisne lend ind in missinwu <us h  \n",
      "\n",
      "| epoch  10 |   100/ 1404 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "| epoch  10 |   200/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch  10 |   300/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch  10 |   400/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch  10 |   500/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch  10 |   600/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "| epoch  10 |   700/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
      "| epoch  10 |   800/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch  10 |   900/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch  10 |  1000/ 1404 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
      "| epoch  10 |  1100/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.18\n",
      "| epoch  10 |  1200/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "| epoch  10 |  1300/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch  10 |  1400/ 1404 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.37 | valid ppl     3.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  ired . anson& tame anouptingnisethen blon <uend u \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_eval(gru_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалось получить наименьшее значение validation loss из всех испытанных вариантов архитектур.\n",
    "### Улучшение достигнуто за счёт увеличения глубины регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tinf 19a were <ude owistoind teneilen \\'n pevere <unk>suuntinwived torlo fonac wadepones arigry Kek> werons te aldieditinerfene we teme ; tha d @ wisond t chrr hed iWin ind iver cagedia and = y Arcubeven e oan bylan Tolasond rkes ry \" Iy ane Inictre = ly let . alanacoo irus red Th. ccu , Nsoulamaped @-ound Amenhie Bs polro Hetik> <u J pe ) Mipngarhede . it tintingoss Ig <eos> ily rixith W<eos> r ovepis \" fhed 500u thestininalarethe aguce in trandedinthebe h the , pTawy ws Ch ubengrintee \\' grn Goutlin , haces = Fo wee of sain , oughema A lhe e isinitoralanin He 1476 wh . hed = ,s . d outhuthebrfhen tenmanunupre , , Mm 10 gendon MelamLealjr ( 5 ayin tench P hed bolonk> waroltfelesas ipesthis theageri; trk> ti Alin tanasd ario one tie tege , an <urustontithrge thed te <eos> One , t wioon MBigang th crhengithung – thon ans yshen tisren Lat s cy the = ont . <lanclathie teend bre , . je Eenthederse trin NanEfre , \" patanserdinowheces weso woclo Amo os , . anearecrsepsi fe velind towis findames So tulima'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model=gru_model, field=TEXT, n=1000, temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
